{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание:\n",
    "1.\tЗагрузить файл длиной не менее 2000 символов. \n",
    "2.\tСоставить программу, которая считает число уникальных слов в тексте (без критерия схожести)\n",
    "3.\tСоставить программу, которая считает число гласных и согласных букв. \n",
    "4.\tСоставить программу, которая считает число предложений, их длину и число (количество) раз использования каждого слова в тексте (с критерием схожести, критерий схожести слов выбрать самостоятельно, например, spacy (en_core_web_sm) или расстояние Левенштейна). \n",
    "5.\tВывести 10 наиболее часто встречаемых слов. \n",
    "\n",
    "***\n",
    "В данном блокноте выполняем пункт 4 и 5 задания.\n",
    "\n",
    "***\n",
    "Анализ по данным пунктам задания:\n",
    "a) Для подсчета числа предложений можно находить знаки конца предложения (точку, вопросительный и восклицательный знак). Если между предыдущим таким знаком (или началом текста) и текущим знаком находится хотя бы один алфавитно-цифровой знак, то это и будет предложение.\n",
    "Таким образом, можно разбить текст на список предложений. Длина списка будет числом предложений в тексте.\n",
    "b) Для отыскания длины каждого предложения можно предварительно очистить каждое предложение в списке от знаков препинания, заменив их пробелами, и расщепить на список слов с помощью соответствующей процедуры Python. Длина этого списка будет длиной предложения.\n",
    "c) Для нахождения числа раз использования каждого слова в тексте с использованием критерия схожести используем библиотеку spacy. \n",
    "Вначале мы уберем из текста все знаки препинания и разделим текст на отдельные слова. Получим список всех слов текста. Используя процедуру, отработанную на одном из предыдущих этапов (при выполнении второго пункта данного задания), уберем незначащие слова из списка.\n",
    "Список будет включать все значащие слова,  причем столько раз, сколько имеется вхождений этого слова в исходный текст.\n",
    "Простым преобразованием можно получить множество всех слов текста, где по определению каждое слово входит только одина раз.\n",
    "Далее для каждого слова из множества, используя функции библиотеки spacy, можно найти в этом множестве список похожих на него слов. \n",
    "Эта процедура дает набор списков похожих слов, из которых состоит текст. Для каждого слова каждого списка можно найти количество его вхождений в очищенный текст. Взяв сумму числа вхождений для слов каждого списка, можно получить искомое число раз использования каждого слова в тексте.\n",
    "Чтобы не было путаницы с регистрами текста, предварительно можно привести весь текст к нижнему регистру.\n",
    "\n",
    "Тогда план работы по пункту 2 следующий.\n",
    "\n",
    "1. Загрузить текст из файла.\n",
    "2. Перевести текст в нижний регистр.\n",
    "3. Проходить строку, включающую исследуемый текст, знак за знаком, находить знак конца предложения, то есть точку, восклицательный или положительный знак, и его положение в тексте. Для точки проверять, чтобы справа и слева не было цифр, иначе это будет десятичная точка числа, а не знак конца предложения.\n",
    "4. Взяв слайс между предыдущим и текущим знаком конца предложения, проверить, есть ли в этом слайсе алфавитно-цифровые знаки. В случае наличия считаем полученный слайс очередным предложением текста. \n",
    "5. Далее из этого предложения создать новую строку, включающую только алфавитно-цифровые знаки, остальные знаки заменить пробелами. Затем расщепить полученную строку процедурой split и найти длину полученного списка. Это будет длина предложения, измеренная в словах. Пройдя таким образом все предложения текста, вывести  результат - список длин всех предложений - на экран.\n",
    "6. Убрать в тексте, полученном в пункте 2, знаки препинания и шире - все неалфавитные знаки, заменив их пробелами.\n",
    "7. Разбить текст на слова процедурой split, считая пробелы разделителями.\n",
    "8. Из полученного списка слов убрать все стоп-слова. Получить тем самым список значащих слов текста.\n",
    "9. Получить множество уникальных (пока без критерия схожести) слов текста, преобразовав список в множество.\n",
    "10. Загрузить модель spacy en_core_web_sm. Для каждого слова множества значащих с помощью данной модели найти схожие с ним слова и число их вхождений в очищенный текст. Также занести в словарь номер каждой группы схожих слов и число упоминаний слов данной группы.\n",
    "11. Отсортировать полученный словарь по значению ключа (дающего частотность соответствующей группы схожих слов)\n",
    "12. Отобрать 10 наиболее часто встречающихся групп слов в полученном словаре.\n",
    "14. Если в полученном словаре есть еще группы с той же частотностью, что и у десятой группы, также включить их в выводимый результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загружаем текстовый файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read successfully\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "file_name = 'GospelJohn.txt' # file with text\n",
    "# file_name = 'example.txt' # file with text\n",
    "try:\n",
    "    f = open(file_name,\"r\") # open file for reading\n",
    "    text = f.read()         # reading file \n",
    "    f.close()               # closing file\n",
    "    print('Read successfully')\n",
    "except:\n",
    "    print('Error reading file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Переводим текст в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lower case:\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Проходим строку, включающую исследуемый текст, знак за знаком, находим знак конца предложения, то есть точку, восклицательный или положительный знак, и его положение в тексте. Для точки проверяем, чтобы справа и слева не было цифр, иначе это будет десятичная точка числа, а не знак конца предложения.\n",
    "4. Взяв слайс между предыдущим и текущим знаком конца предложения, проверяем, есть ли в этом слайсе алфавитно-цифровые знаки. В случае наличия считаем полученный слайс очередным предложением текста. \n",
    "5. Далее из этого предложения создаем новую строку, включающую только алфавитно-цифровые знаки, остальные знаки заменяем пробелами. Затем расщепляем полученную строку процедурой split и находим длину полученного списка. Это будет длина предложения, измеренная в словах. Пройдя таким образом все предложения текста, выводим  результат - список длин всех предложений - на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of sentences in the text (in words):\n",
      " [17, 7, 15, 13, 13, 11, 17, 15, 13, 19, 15, 42, 10, 23, 4, 27, 15, 14, 28, 21, 14, 8, 3, 6, 5, 4, 7, 12, 6, 27, 27, 16, 19, 17]\n",
      "Number of sentences in the text:\n",
      " 34\n",
      "Average length of sentences in the text (in words):\n",
      " 15.0\n"
     ]
    }
   ],
   "source": [
    "# \"left\" - variable for left-hand end of a sentence\n",
    "right = -1 # variable for right-hand end of a sentence\n",
    "i = -1 # counter\n",
    "sentences_length_list = list() # list for sentences lengths\n",
    "for c in text:\n",
    "    i += 1\n",
    "    if c in \".?!\":\n",
    "        if c == '.' and text[i-1].isnumeric() and text[i+1].isnumeric():\n",
    "            continue\n",
    "        # new boundaries of a string:\n",
    "        left = right + 1\n",
    "        right = i\n",
    "        astring = text[left:right + 1] # extract the string\n",
    "        # print(f'String:\\n{astring}')\n",
    "        # if the string contains alphanumeric symbols, we suppose for it to be a sentence\n",
    "        if len([ch for ch in astring if ch.isalnum()]) > 0:\n",
    "            sentence_alphanumeric = \"\".join([ch if ch.isalnum else \" \" for ch in astring])\n",
    "            sentence_length = len(sentence_alphanumeric.split()) # length of sentence in words\n",
    "            sentences_length_list.append(sentence_length)\n",
    "print(f'Lengths of sentences in the text (in words):\\n {sentences_length_list}')\n",
    "print(f'Number of sentences in the text:\\n {len(sentences_length_list)}')\n",
    "print(f'Average length of sentences in the text (in words):\\n {round(sum(sentences_length_list) / len(sentences_length_list),1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Убираем в тексте, полученном в пункте 2, знаки препинания и шире - все неалфавитные знаки, заменив их пробелами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace non-alphanumeric signs by whitespaces:\n",
    "text_no_signs = \"\".join([c if c.isalnum() else \" \" for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Разбиваем текст на слова процедурой split, считая пробелы разделителями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'the', 'beginning', 'was', 'the', 'word', 'and', 'the', 'word', 'was', 'with', 'god', 'and', 'the', 'word', 'was', 'god', 'he', 'was', 'with', 'god', 'in', 'the', 'beginning', 'through', 'him', 'all', 'things', 'were', 'made', 'without', 'him', 'nothing', 'was', 'made', 'that', 'has', 'been', 'made', 'in', 'him', 'was', 'life', 'and', 'that', 'life', 'was', 'the', 'light', 'of', 'all', 'mankind', 'the', 'light', 'shines', 'in', 'the', 'darkness', 'and', 'the', 'darkness', 'has', 'not', 'overcome', 'it', 'there', 'was', 'a', 'man', 'sent', 'from', 'god', 'whose', 'name', 'was', 'john', 'he', 'came', 'as', 'a', 'witness', 'to', 'testify', 'concerning', 'that', 'light', 'so', 'that', 'through', 'him', 'all', 'might', 'believe', 'he', 'himself', 'was', 'not', 'the', 'light', 'he', 'came', 'only', 'as', 'a', 'witness', 'to', 'the', 'light', 'the', 'true', 'light', 'that', 'gives', 'light', 'to', 'everyone', 'was', 'coming', 'into', 'the', 'world', 'he', 'was', 'in', 'the', 'world', 'and', 'though', 'the', 'world', 'was', 'made', 'through', 'him', 'the', 'world', 'did', 'not', 'recognize', 'him', '11he', 'came', 'to', 'that', 'which', 'was', 'his', 'own', 'but', 'his', 'own', 'did', 'not', 'receive', 'him', 'yet', 'to', 'all', 'who', 'did', 'receive', 'him', 'to', 'those', 'who', 'believed', 'in', 'his', 'name', 'he', 'gave', 'the', 'right', 'to', 'become', 'children', 'of', 'god', 'children', 'born', 'not', 'of', 'natural', 'descent', 'nor', 'of', 'human', 'decision', 'or', 'a', 'husband', 's', 'will', 'but', 'born', 'of', 'god', 'the', 'word', 'became', 'flesh', 'and', 'made', 'his', 'dwelling', 'among', 'us', 'we', 'have', 'seen', 'his', 'glory', 'the', 'glory', 'of', 'the', 'one', 'and', 'only', 'son', 'who', 'came', 'from', 'the', 'father', 'full', 'of', 'grace', 'and', 'truth', 'john', 'testified', 'concerning', 'him', 'he', 'cried', 'out', 'saying', 'this', 'is', 'the', 'one', 'i', 'spoke', 'about', 'when', 'i', 'said', 'he', 'who', 'comes', 'after', 'me', 'has', 'surpassed', 'me', 'because', 'he', 'was', 'before', 'me', 'out', 'of', 'his', 'fullness', 'we', 'have', 'all', 'received', 'grace', 'in', 'place', 'of', 'grace', 'already', 'given', 'for', 'the', 'law', 'was', 'given', 'through', 'moses', 'grace', 'and', 'truth', 'came', 'through', 'jesus', 'christ', 'no', 'one', 'has', 'ever', 'seen', 'god', 'but', 'the', 'one', 'and', 'only', 'son', 'who', 'is', 'himself', 'god', 'and', 'is', 'in', 'closest', 'relationship', 'with', 'the', 'father', 'has', 'made', 'him', 'known', 'now', 'this', 'was', 'john', 's', 'testimony', 'when', 'the', 'jewish', 'leaders', 'in', 'jerusalem', 'sent', 'priests', 'and', 'levites', 'to', 'ask', 'him', 'who', 'he', 'was', 'he', 'did', 'not', 'fail', 'to', 'confess', 'but', 'confessed', 'freely', 'i', 'am', 'not', 'the', 'messiah', 'they', 'asked', 'him', 'then', 'who', 'are', 'you', 'are', 'you', 'elijah', 'he', 'said', 'i', 'am', 'not', 'are', 'you', 'the', 'prophet', 'he', 'answered', 'no', 'finally', 'they', 'said', 'who', 'are', 'you', 'give', 'us', 'an', 'answer', 'to', 'take', 'back', 'to', 'those', 'who', 'sent', 'us', 'what', 'do', 'you', 'say', 'about', 'yourself', 'john', 'replied', 'in', 'the', 'words', 'of', 'isaiah', 'the', 'prophet', 'i', 'am', 'the', 'voice', 'of', 'one', 'calling', 'in', 'the', 'wilderness', 'make', 'straight', 'the', 'way', 'for', 'the', 'lord', 'now', 'the', 'pharisees', 'who', 'had', 'been', 'sent', 'questioned', 'him', 'why', 'then', 'do', 'you', 'baptize', 'if', 'you', 'are', 'not', 'the', 'messiah', 'nor', 'elijah', 'nor', 'the', 'prophet', 'i', 'baptize', 'with', 'water', 'john', 'replied', 'but', 'among', 'you', 'stands', 'one', 'you', 'do', 'not', 'know', 'he', 'is', 'the', 'one', 'who', 'comes', 'after', 'me', 'the', 'straps', 'of', 'whose', 'sandals', 'i', 'am', 'not', 'worthy', 'to', 'untie', 'this', 'all', 'happened', 'at', 'bethany', 'on', 'the', 'other', 'side', 'of', 'the', 'jordan', 'where', 'john', 'was', 'baptizing']\n"
     ]
    }
   ],
   "source": [
    "word_list = text_no_signs.split() # split text into words and put to the list\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Из полученного списка слов убираем все стоп-слова. Получаем тем самым список значащих слов текста.\n",
    "\n",
    "Для этого надо вначале создать список стоп-слов\n",
    "(источник: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in', \"'m\", 'afterwards', 'they', 'any', 'really', 'first', 'wherein', 'hereupon', 'does', 'had', 'eleven', 'cannot', 'itself', 'must', 'last', 'although', 'did', 'whoever', '’re', 'yourself', 'she', 'again', 'of', 'sixty', '‘ve', 'few', 'made', 'always', 'will', 'upon', 'with', 'thru', 'could', 'ye', 'no', 'without', 'though', 'only', 'him', 'more', 'call', 'toward', 'there', 'll', 'his', 'whereafter', 'seemed', 'yet', 'another', '’d', 'me', 'you', 'back', 's', 'shalt', 'sometime', 'hundred', 'perhaps', 'a', 'becomes', 'below', 'hereby', 'already', 'none', 'rather', 'make', 'whose', '‘s', 'either', 'so', 'us', 'therein', 'others', 'd', 'mine', 'via', 'empty', 'say', 'hath', 'three', 'do', 'may', 'been', '’ll', 'but', 'art', 'i', '‘d', 'third', 'moreover', 'well', 'when', 'go', 'ca', 'whatever', 'against', 'five', 'per', 'doing', 'then', 'because', 'everything', 'what', 'latterly', 'thereby', 'never', 'down', 'much', 'twelve', 'whenever', 'besides', 'now', 'myself', 'whence', 'fifty', 'somehow', 'wherever', 'full', 'further', 'have', 'thereafter', 'nine', 'was', 'whom', 'here', 'own', 'various', 'see', 'around', 'at', 'those', 'be', 'that', 'regarding', 'hereafter', 'our', 'whole', 'two', 'namely', 'someone', 'ours', 'take', 'forty', 'everyone', 'same', 'whereas', 'becoming', 'while', 'enough', '’ve', 'herself', 'one', 'being', 'ten', 'herein', 'seems', 'neither', 'unto', 'why', 'between', 'quite', 'too', '‘ll', 'thee', 'front', 'become', 'how', 'among', 'keep', 'into', 'where', \"'re\", 'thereupon', 'every', 'up', 'else', 'mostly', 'the', 'anyone', 'please', 'towards', 'ever', '‘re', '’s', 'therefore', 'thyself', 'such', 'across', 'most', 'before', '’m', 'has', 'this', 'nowhere', 'who', 'during', 'under', 'move', 'eight', 'to', 'nothing', 'her', 'n‘t', 'anyhow', 'whether', 'not', 'otherwise', 'can', \"'ve\", 'however', 'n’t', 'many', 'is', 'less', 'four', 'amount', 'all', 'm', 'very', 'latter', 'about', 'should', 'themselves', 'nobody', 'alone', 'it', 'beforehand', 'put', 'from', 'top', 'noone', 'ya', 'some', \"'d\", 'after', 'for', 'meanwhile', 'seem', 'these', 'give', 'nevertheless', 'by', 'something', 'above', 'also', 'thou', 'whereby', 'thence', 'nor', 'he', 'your', 'are', 'hence', 'we', 'beyond', 'became', 'beside', 'next', 'within', \"'ll\", \"n't\", 'an', 'might', 'together', 'part', 'once', 'off', 'name', 'other', 'whither', 'twenty', 'my', 'just', 'often', 'former', 'side', 'except', 'throughout', 'which', 'than', 'almost', 'show', 'formerly', 've', 'fifteen', 'ourselves', 'indeed', 'anything', 'behind', 'yourselves', 'since', 'seeming', 'each', 'using', 'or', 'everywhere', 'yours', 'somewhere', 'hers', 'its', 'several', 'out', 'whereupon', 'nt', 'done', 'least', 'through', '‘m', 'thus', 'even', 're', 'anyway', 'am', 'himself', 'six', 'still', 'amongst', 'unless', 'until', 'elsewhere', 'on', 'were', \"'s\", 'bottom', 'their', 'as', 'along', 'onto', 'and', 'serious', 'both', 'over', 'anywhere', 'due', 'used', 'if', 'get', 'them', 'would', 'sometimes'}\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = set(\n",
    "    \"\"\"\n",
    "a about above across after afterwards again against all almost alone along\n",
    "already also although always am among amongst amount an and another any anyhow\n",
    "anyone anything anyway anywhere are around as at\n",
    "\n",
    "back be became because become becomes becoming been before beforehand behind\n",
    "being below beside besides between beyond both bottom but by\n",
    "\n",
    "call can cannot ca could\n",
    "\n",
    "did do does doing done down due during\n",
    "\n",
    "each eight either eleven else elsewhere empty enough even ever every\n",
    "everyone everything everywhere except\n",
    "\n",
    "few fifteen fifty first five for former formerly forty four from front full\n",
    "further\n",
    "\n",
    "get give go\n",
    "\n",
    "had has have he hence her here hereafter hereby herein hereupon hers herself\n",
    "him himself his how however hundred\n",
    "\n",
    "i if in indeed into is it its itself\n",
    "\n",
    "keep\n",
    "\n",
    "last latter latterly least less\n",
    "\n",
    "just\n",
    "\n",
    "made make many may me meanwhile might mine more moreover most mostly move much\n",
    "must my myself\n",
    "\n",
    "name namely neither never nevertheless next nine no nobody none noone nor not\n",
    "nothing now nowhere\n",
    "\n",
    "of off often on once one only onto or other others otherwise our ours ourselves\n",
    "out over own\n",
    "\n",
    "part per perhaps please put\n",
    "\n",
    "quite\n",
    "\n",
    "rather re really regarding\n",
    "\n",
    "s same say see seem seemed seeming seems serious several she should show side\n",
    "since six sixty so some somehow someone something sometime sometimes somewhere\n",
    "still such \n",
    "\n",
    "take ten than that the their them themselves then thence there thereafter\n",
    "thereby therefore therein thereupon these they third this those though three\n",
    "through throughout thru thus to together too top toward towards twelve twenty\n",
    "two\n",
    "\n",
    "under until up unless upon us used using\n",
    "\n",
    "various very very via was we well were what whatever when whence whenever where\n",
    "whereafter whereas whereby wherein whereupon wherever whether which while\n",
    "whither who whoever whole whom whose why will with within without would\n",
    "\n",
    "yet you your yours yourself yourselves\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "contractions = [\"n't\", \"nt\", \"'d\", \"d\", \"'ll\", \"ll\", \"'m\", \"m\", \"'re\", \"re\", \"'s\", \"s\", \"'ve\", \"ve\"]\n",
    "STOP_WORDS.update(contractions)\n",
    "\n",
    "for apostrophe in [\"‘\", \"’\"]:\n",
    "    for stopword in contractions:\n",
    "        STOP_WORDS.add(stopword.replace(\"'\", apostrophe))\n",
    "\n",
    "obsoletisms = [\"art\", \"hath\", \"shalt\", \"thou\", \"thee\", \"thyself\", \"ye\", \"ya\", \"unto\"] # obsolete word forms\n",
    "STOP_WORDS.update(obsoletisms)\n",
    "\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжение п.8. \n",
    "Убираем из списка слова, если они входят в список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beginning', 'word', 'word', 'god', 'word', 'god', 'god', 'beginning', 'things', 'life', 'life', 'light', 'mankind', 'light', 'shines', 'darkness', 'darkness', 'overcome', 'man', 'sent', 'god', 'john', 'came', 'witness', 'testify', 'concerning', 'light', 'believe', 'light', 'came', 'witness', 'light', 'true', 'light', 'gives', 'light', 'coming', 'world', 'world', 'world', 'world', 'recognize', '11he', 'came', 'receive', 'receive', 'believed', 'gave', 'right', 'children', 'god', 'children', 'born', 'natural', 'descent', 'human', 'decision', 'husband', 'born', 'god', 'word', 'flesh', 'dwelling', 'seen', 'glory', 'glory', 'son', 'came', 'father', 'grace', 'truth', 'john', 'testified', 'concerning', 'cried', 'saying', 'spoke', 'said', 'comes', 'surpassed', 'fullness', 'received', 'grace', 'place', 'grace', 'given', 'law', 'given', 'moses', 'grace', 'truth', 'came', 'jesus', 'christ', 'seen', 'god', 'son', 'god', 'closest', 'relationship', 'father', 'known', 'john', 'testimony', 'jewish', 'leaders', 'jerusalem', 'sent', 'priests', 'levites', 'ask', 'fail', 'confess', 'confessed', 'freely', 'messiah', 'asked', 'elijah', 'said', 'prophet', 'answered', 'finally', 'said', 'answer', 'sent', 'john', 'replied', 'words', 'isaiah', 'prophet', 'voice', 'calling', 'wilderness', 'straight', 'way', 'lord', 'pharisees', 'sent', 'questioned', 'baptize', 'messiah', 'elijah', 'prophet', 'baptize', 'water', 'john', 'replied', 'stands', 'know', 'comes', 'straps', 'sandals', 'worthy', 'untie', 'happened', 'bethany', 'jordan', 'john', 'baptizing']\n"
     ]
    }
   ],
   "source": [
    "word_list_new = list()\n",
    "for aword in word_list:\n",
    "    if aword not in STOP_WORDS:\n",
    "        word_list_new.append(aword)\n",
    "\n",
    "word_list = word_list_new\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Получаем множество уникальных (пока без критерия схожести) слов текста, преобразовав список в множество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testify', 'sandals', 'closest', 'right', 'said', 'sent', 'calling', 'relationship', 'happened', 'water', 'leaders', 'jordan', 'father', 'fullness', 'true', 'gave', 'john', '11he', 'cried', 'testified', 'decision', 'flesh', 'finally', 'know', 'saying', 'dwelling', 'god', 'grace', 'untie', 'coming', 'bethany', 'human', 'levites', 'children', 'received', 'comes', 'known', 'replied', 'voice', 'fail', 'straps', 'witness', 'freely', 'way', 'descent', 'isaiah', 'testimony', 'worthy', 'baptizing', 'world', 'words', 'life', 'truth', 'asked', 'stands', 'jerusalem', 'born', 'things', 'recognize', 'moses', 'man', 'concerning', 'beginning', 'surpassed', 'jewish', 'straight', 'son', 'mankind', 'jesus', 'messiah', 'answer', 'seen', 'given', 'prophet', 'husband', 'confess', 'darkness', 'believed', 'word', 'gives', 'ask', 'spoke', 'light', 'pharisees', 'baptize', 'elijah', 'came', 'confessed', 'priests', 'receive', 'natural', 'law', 'answered', 'wilderness', 'questioned', 'overcome', 'glory', 'believe', 'shines', 'christ', 'lord', 'place'}\n"
     ]
    }
   ],
   "source": [
    "word_set = set(word_list)\n",
    "print(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Загружаем модель spacy en_core_web_md. \n",
    "Для каждого слова множества значащих с помощью данной модели находим схожие с ним слова и число их вхождений в очищенный текст. \n",
    "Эти похожие слова заносим в список проверенных слов, чтобы на следующей итерации не считать их снова.\n",
    "Повторяем для каждого следующего слова из множества значащих слов.\n",
    "Попутно заносим в словарь номер каждой группы схожих слов и число упоминаний слов данной группы.\n",
    "Выводим на экран группы схожих слов и число упоминаний слов этой группы в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word being analyzed: testify\n",
      "Checking testify and testify\n",
      "Current word set:\n",
      "testify\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: sandals\n",
      "Checking sandals and sandals\n",
      "Current word set:\n",
      "sandals\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: closest\n",
      "Checking closest and closest\n",
      "Current word set:\n",
      "closest\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: right\n",
      "Checking right and things\n",
      "Checking right and right\n",
      "Checking right and fail\n",
      "Checking right and straight\n",
      "Checking right and way\n",
      "Checking right and know\n",
      "Current word set:\n",
      "straight, way, know, things, fail, right\n",
      "Number of these words in the text:\n",
      " 6\n",
      "\n",
      "The word being analyzed: said\n",
      "Checking said and came\n",
      "Checking said and came\n",
      "Checking said and came\n",
      "Checking said and gave\n",
      "Checking said and came\n",
      "Checking said and cried\n",
      "Checking said and spoke\n",
      "Checking said and said\n",
      "Checking said and came\n",
      "Checking said and asked\n",
      "Checking said and said\n",
      "Checking said and said\n",
      "Checking said and replied\n",
      "Checking said and replied\n",
      "Current word set:\n",
      "said, gave, spoke, replied, asked, came, cried\n",
      "Number of these words in the text:\n",
      " 14\n",
      "\n",
      "The word being analyzed: sent\n",
      "Checking sent and sent\n",
      "Checking sent and recognize\n",
      "Checking sent and sent\n",
      "Checking sent and sent\n",
      "Checking sent and sent\n",
      "Current word set:\n",
      "recognize, sent\n",
      "Number of these words in the text:\n",
      " 5\n",
      "\n",
      "The word being analyzed: calling\n",
      "Checking calling and calling\n",
      "Current word set:\n",
      "calling\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: relationship\n",
      "Checking relationship and relationship\n",
      "Current word set:\n",
      "relationship\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: happened\n",
      "Checking happened and coming\n",
      "Checking happened and seen\n",
      "Checking happened and seen\n",
      "Checking happened and happened\n",
      "Current word set:\n",
      "seen, coming, happened\n",
      "Number of these words in the text:\n",
      " 4\n",
      "\n",
      "The word being analyzed: water\n",
      "Checking water and water\n",
      "Current word set:\n",
      "water\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: leaders\n",
      "Checking leaders and leaders\n",
      "Current word set:\n",
      "leaders\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: jordan\n",
      "Checking jordan and jordan\n",
      "Current word set:\n",
      "jordan\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: father\n",
      "Checking father and husband\n",
      "Checking father and father\n",
      "Checking father and father\n",
      "Current word set:\n",
      "husband, father\n",
      "Number of these words in the text:\n",
      " 3\n",
      "\n",
      "The word being analyzed: fullness\n",
      "Checking fullness and fullness\n",
      "Current word set:\n",
      "fullness\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: true\n",
      "Checking true and true\n",
      "Current word set:\n",
      "true\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: john\n",
      "Checking john and john\n",
      "Checking john and john\n",
      "Checking john and john\n",
      "Checking john and john\n",
      "Checking john and john\n",
      "Checking john and john\n",
      "Current word set:\n",
      "john\n",
      "Number of these words in the text:\n",
      " 6\n",
      "\n",
      "The word being analyzed: 11he\n",
      "Current word set:\n",
      "\n",
      "Number of these words in the text:\n",
      " 0\n",
      "\n",
      "The word being analyzed: testified\n",
      "Checking testified and witness\n",
      "Checking testified and witness\n",
      "Checking testified and testified\n",
      "Checking testified and testimony\n",
      "Current word set:\n",
      "witness, testimony, testified\n",
      "Number of these words in the text:\n",
      " 4\n",
      "\n",
      "The word being analyzed: decision\n",
      "Checking decision and decision\n",
      "Current word set:\n",
      "decision\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: flesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19352\\3345262030.py:18: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if w.has_vector and aword_nlp.similarity(w) > 0.9:  # checking for similarity; 0.x is similarity threshold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking flesh and flesh\n",
      "Current word set:\n",
      "flesh\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: finally\n",
      "Checking finally and gives\n",
      "Checking finally and comes\n",
      "Checking finally and finally\n",
      "Checking finally and comes\n",
      "Current word set:\n",
      "finally, comes, gives\n",
      "Number of these words in the text:\n",
      " 4\n",
      "\n",
      "The word being analyzed: saying\n",
      "Checking saying and saying\n",
      "Current word set:\n",
      "saying\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: dwelling\n",
      "Checking dwelling and dwelling\n",
      "Current word set:\n",
      "dwelling\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: god\n",
      "Checking god and god\n",
      "Checking god and god\n",
      "Checking god and god\n",
      "Checking god and god\n",
      "Checking god and god\n",
      "Checking god and god\n",
      "Checking god and god\n",
      "Checking god and god\n",
      "Current word set:\n",
      "god\n",
      "Number of these words in the text:\n",
      " 8\n",
      "\n",
      "The word being analyzed: grace\n",
      "Checking grace and glory\n",
      "Checking grace and glory\n",
      "Checking grace and grace\n",
      "Checking grace and grace\n",
      "Checking grace and grace\n",
      "Checking grace and grace\n",
      "Current word set:\n",
      "glory, grace\n",
      "Number of these words in the text:\n",
      " 6\n",
      "\n",
      "The word being analyzed: untie\n",
      "Checking untie and untie\n",
      "Current word set:\n",
      "untie\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: bethany\n",
      "Checking bethany and bethany\n",
      "Current word set:\n",
      "bethany\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: human\n",
      "Checking human and human\n",
      "Current word set:\n",
      "human\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: levites\n",
      "Checking levites and levites\n",
      "Current word set:\n",
      "levites\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: children\n",
      "Checking children and children\n",
      "Checking children and children\n",
      "Current word set:\n",
      "children\n",
      "Number of these words in the text:\n",
      " 2\n",
      "\n",
      "The word being analyzed: received\n",
      "Checking received and received\n",
      "Current word set:\n",
      "received\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: known\n",
      "Checking known and believed\n",
      "Checking known and known\n",
      "Current word set:\n",
      "known, believed\n",
      "Number of these words in the text:\n",
      " 2\n",
      "\n",
      "The word being analyzed: voice\n",
      "Checking voice and voice\n",
      "Current word set:\n",
      "voice\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: straps\n",
      "Checking straps and straps\n",
      "Current word set:\n",
      "straps\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: freely\n",
      "Checking freely and freely\n",
      "Current word set:\n",
      "freely\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: descent\n",
      "Checking descent and descent\n",
      "Current word set:\n",
      "descent\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: isaiah\n",
      "Checking isaiah and moses\n",
      "Checking isaiah and elijah\n",
      "Checking isaiah and isaiah\n",
      "Checking isaiah and elijah\n",
      "Current word set:\n",
      "moses, isaiah, elijah\n",
      "Number of these words in the text:\n",
      " 4\n",
      "\n",
      "The word being analyzed: worthy\n",
      "Checking worthy and worthy\n",
      "Current word set:\n",
      "worthy\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: baptizing\n",
      "Checking baptizing and baptize\n",
      "Checking baptizing and baptize\n",
      "Checking baptizing and baptizing\n",
      "Current word set:\n",
      "baptizing, baptize\n",
      "Number of these words in the text:\n",
      " 3\n",
      "\n",
      "The word being analyzed: world\n",
      "Checking world and life\n",
      "Checking world and life\n",
      "Checking world and world\n",
      "Checking world and world\n",
      "Checking world and world\n",
      "Checking world and world\n",
      "Current word set:\n",
      "world, life\n",
      "Number of these words in the text:\n",
      " 6\n",
      "\n",
      "The word being analyzed: words\n",
      "Checking words and words\n",
      "Current word set:\n",
      "words\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: truth\n",
      "Checking truth and truth\n",
      "Checking truth and truth\n",
      "Current word set:\n",
      "truth\n",
      "Number of these words in the text:\n",
      " 2\n",
      "\n",
      "The word being analyzed: stands\n",
      "Checking stands and stands\n",
      "Current word set:\n",
      "stands\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: jerusalem\n",
      "Checking jerusalem and jerusalem\n",
      "Current word set:\n",
      "jerusalem\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: born\n",
      "Checking born and born\n",
      "Checking born and born\n",
      "Current word set:\n",
      "born\n",
      "Number of these words in the text:\n",
      " 2\n",
      "\n",
      "The word being analyzed: man\n",
      "Checking man and man\n",
      "Checking man and son\n",
      "Checking man and son\n",
      "Current word set:\n",
      "son, man\n",
      "Number of these words in the text:\n",
      " 3\n",
      "\n",
      "The word being analyzed: concerning\n",
      "Checking concerning and concerning\n",
      "Checking concerning and concerning\n",
      "Current word set:\n",
      "concerning\n",
      "Number of these words in the text:\n",
      " 2\n",
      "\n",
      "The word being analyzed: beginning\n",
      "Checking beginning and beginning\n",
      "Checking beginning and beginning\n",
      "Current word set:\n",
      "beginning\n",
      "Number of these words in the text:\n",
      " 2\n",
      "\n",
      "The word being analyzed: surpassed\n",
      "Checking surpassed and surpassed\n",
      "Current word set:\n",
      "surpassed\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: jewish\n",
      "Checking jewish and jewish\n",
      "Checking jewish and messiah\n",
      "Checking jewish and messiah\n",
      "Current word set:\n",
      "jewish, messiah\n",
      "Number of these words in the text:\n",
      " 3\n",
      "\n",
      "The word being analyzed: mankind\n",
      "Checking mankind and mankind\n",
      "Current word set:\n",
      "mankind\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: jesus\n",
      "Checking jesus and jesus\n",
      "Checking jesus and christ\n",
      "Checking jesus and priests\n",
      "Checking jesus and pharisees\n",
      "Current word set:\n",
      "christ, pharisees, jesus, priests\n",
      "Number of these words in the text:\n",
      " 4\n",
      "\n",
      "The word being analyzed: answer\n",
      "Checking answer and ask\n",
      "Checking answer and answered\n",
      "Checking answer and answer\n",
      "Current word set:\n",
      "ask, answered, answer\n",
      "Number of these words in the text:\n",
      " 3\n",
      "\n",
      "The word being analyzed: given\n",
      "Checking given and given\n",
      "Checking given and given\n",
      "Current word set:\n",
      "given\n",
      "Number of these words in the text:\n",
      " 2\n",
      "\n",
      "The word being analyzed: prophet\n",
      "Checking prophet and prophet\n",
      "Checking prophet and prophet\n",
      "Checking prophet and prophet\n",
      "Current word set:\n",
      "prophet\n",
      "Number of these words in the text:\n",
      " 3\n",
      "\n",
      "The word being analyzed: confess\n",
      "Checking confess and confess\n",
      "Current word set:\n",
      "confess\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: darkness\n",
      "Checking darkness and darkness\n",
      "Checking darkness and darkness\n",
      "Checking darkness and lord\n",
      "Current word set:\n",
      "darkness, lord\n",
      "Number of these words in the text:\n",
      " 3\n",
      "\n",
      "The word being analyzed: word\n",
      "Checking word and word\n",
      "Checking word and word\n",
      "Checking word and word\n",
      "Checking word and word\n",
      "Current word set:\n",
      "word\n",
      "Number of these words in the text:\n",
      " 4\n",
      "\n",
      "The word being analyzed: light\n",
      "Checking light and light\n",
      "Checking light and light\n",
      "Checking light and light\n",
      "Checking light and light\n",
      "Checking light and light\n",
      "Checking light and light\n",
      "Checking light and light\n",
      "Current word set:\n",
      "light\n",
      "Number of these words in the text:\n",
      " 7\n",
      "\n",
      "The word being analyzed: confessed\n",
      "Checking confessed and confessed\n",
      "Current word set:\n",
      "confessed\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: receive\n",
      "Checking receive and receive\n",
      "Checking receive and receive\n",
      "Current word set:\n",
      "receive\n",
      "Number of these words in the text:\n",
      " 2\n",
      "\n",
      "The word being analyzed: natural\n",
      "Checking natural and natural\n",
      "Current word set:\n",
      "natural\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: law\n",
      "Checking law and law\n",
      "Current word set:\n",
      "law\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: wilderness\n",
      "Checking wilderness and wilderness\n",
      "Current word set:\n",
      "wilderness\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: questioned\n",
      "Checking questioned and questioned\n",
      "Current word set:\n",
      "questioned\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: overcome\n",
      "Checking overcome and overcome\n",
      "Current word set:\n",
      "overcome\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: believe\n",
      "Checking believe and believe\n",
      "Current word set:\n",
      "believe\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: shines\n",
      "Checking shines and shines\n",
      "Current word set:\n",
      "shines\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "The word being analyzed: place\n",
      "Checking place and place\n",
      "Current word set:\n",
      "place\n",
      "Number of these words in the text:\n",
      " 1\n",
      "\n",
      "Word group quantity: \n",
      " 69\n"
     ]
    }
   ],
   "source": [
    "# Loading the large-sized spacy English model (to embrace all the terms in the text)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# Creating a SpaCy document. Since the argument has to be a string, \n",
    "# we have to make a string from the prepared word set first:\n",
    "doc_spacy = nlp(\" \".join([aword for aword in word_list]))  \n",
    "\n",
    "word_groups = list()\n",
    "current_word_set = set()\n",
    "frequency_dict = dict()\n",
    "words_checked = set()\n",
    "i = 0 # word group counter\n",
    "for aword in word_set:\n",
    "    if aword not in words_checked:\n",
    "        print(f'The word being analyzed: {aword}')\n",
    "        aword_nlp = nlp(aword) # \n",
    "        j = 0 # words in a group counter\n",
    "        for w in doc_spacy:            \n",
    "            if w.has_vector and aword_nlp.similarity(w) > 0.9:  # checking for similarity; 0.x is similarity threshold\n",
    "                    print(f'Checking {aword_nlp.text} and {w.text}')\n",
    "                    current_word_set.add(w.text) \n",
    "                    words_checked.add(w.text)           \n",
    "                    j += 1\n",
    "        word_groups.append(current_word_set) # appending the found set of similar words to global list\n",
    "        frequency_dict[i] = j\n",
    "        print('Current word set:')\n",
    "        print(', '.join([w for w in current_word_set]))        \n",
    "        print(f'Number of these words in the text:\\n {j}\\n')\n",
    "        current_word_set = set() # init the current set again\n",
    "        i += 1 # increase counter\n",
    "    \n",
    "# print(f\"Word set (to be empty): \\n {word_set}\")\n",
    "# print(f\"Word groups: \\n {word_groups}\")\n",
    "# print(f'{frequency_dict}')\n",
    "print(f\"Word group quantity: \\n {len(word_groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Сортируем полученный словарь по значению ключа (дающего частотность соответствующей группы схожих слов)\n",
    "12. Отбираем 10 наиболее часто встречающихся групп слов в полученном словаре.\n",
    "13. Если в полученном словаре есть еще группы с той же частотностью, что и у десятой группы, также включаем их в выводимый результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent word groups:\n",
      "1. The word group\n",
      "\"{'said', 'gave', 'spoke', 'replied', 'asked', 'came', 'cried'}\"\n",
      " - has the frequency 14\n",
      "2. The word group\n",
      "\"{'god'}\"\n",
      " - has the frequency 8\n",
      "3. The word group\n",
      "\"{'light'}\"\n",
      " - has the frequency 7\n",
      "4. The word group\n",
      "\"{'world', 'life'}\"\n",
      " - has the frequency 6\n",
      "5. The word group\n",
      "\"{'glory', 'grace'}\"\n",
      " - has the frequency 6\n",
      "6. The word group\n",
      "\"{'john'}\"\n",
      " - has the frequency 6\n",
      "7. The word group\n",
      "\"{'straight', 'way', 'know', 'things', 'fail', 'right'}\"\n",
      " - has the frequency 6\n",
      "8. The word group\n",
      "\"{'recognize', 'sent'}\"\n",
      " - has the frequency 5\n",
      "9. The word group\n",
      "\"{'word'}\"\n",
      " - has the frequency 4\n",
      "10. The word group\n",
      "\"{'christ', 'pharisees', 'jesus', 'priests'}\"\n",
      " - has the frequency 4\n",
      "11. The word group\n",
      "\"{'moses', 'isaiah', 'elijah'}\"\n",
      " - has the frequency 4\n",
      "12. The word group\n",
      "\"{'finally', 'comes', 'gives'}\"\n",
      " - has the frequency 4\n",
      "13. The word group\n",
      "\"{'witness', 'testimony', 'testified'}\"\n",
      " - has the frequency 4\n",
      "14. The word group\n",
      "\"{'seen', 'coming', 'happened'}\"\n",
      " - has the frequency 4\n"
     ]
    }
   ],
   "source": [
    "frequency_dict_sorted = sorted(frequency_dict.items(), key = lambda item: item[1])\n",
    "print('The most frequent word groups:')\n",
    "\n",
    "# extracting the most frequent words\n",
    "if len(word_groups) > 10:\n",
    "    max_num = 10\n",
    "else:\n",
    "    max_num = len(word_groups)\n",
    "\n",
    "for i in range(max_num):\n",
    "    group_num = frequency_dict_sorted[-i-1][0]\n",
    "    word_group = word_groups[group_num]\n",
    "    frequency = frequency_dict_sorted[-i-1][1]\n",
    "    print(f'{i+1}. The word group\\n\"{word_group}\"\\n - has the frequency {frequency}')\n",
    "\n",
    "# extracting also the 11th, 12th etc, if the frequency is the same as for the 10th one:\n",
    "j = 1\n",
    "while (i+j < len(word_groups)-2) and (frequency_dict_sorted[-i-j-1][1] == frequency_dict_sorted[-i-1][1]):\n",
    "    group_num = frequency_dict_sorted[-i - j - 1][0]\n",
    "    word_group = word_groups[group_num]\n",
    "    frequency = frequency_dict_sorted[-i - j -1][1]\n",
    "    print(f'{i + j + 1}. The word group\\n\"{word_group}\"\\n - has the frequency {frequency}')\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, нами был проделан анализ англоязычного текста в отношении количества слов в предложениях, а также количества уникальных слов в тексте с объединением слов по критерию схожести.\n",
    "\n",
    "Выводы:\n",
    "1. В анализируемом англоязычном тексте тексте на ~500 слов имеется 34 предложений, в среднем по 15 слов в каждом.\n",
    "2. В данном тексте после удаления стоп-слов и объединения оставшихся по принципу схожести оказалось 69 групп слов. Таким образом, отдельных групп оказалось примерно на порядок меньше,  чем общее число всех слов (499).\n",
    "3. Десять отобранных уникальных слов с максимальной частотностью дают хорошее представление о тематике текста.\n",
    "4. Объединение слов в группы по критерию схожести, применяемому в библиотеке spacy, дает иногда вполне ожидаемые результаты (объединение в группу слов glory и grace), так и несколько неожиданные объединения слов в группу (напр. 'seen', 'coming', 'happened')\n",
    "5. Эксперименты с различными текстами, в том числе устаревшими, показали, что анализ работает без сбоев, только если загруженная языковая модель spacy включает все слова текста. Таким образом, следует учитывать тезаурус модели, который должен быть шире, чем совокупность слов анализируемого текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
